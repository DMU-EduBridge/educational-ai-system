#!/usr/bin/env python3
"""
íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Educational AI Systemì˜ ì „ì²´ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import sys
import tempfile
import json
from pathlib import Path
from typing import Dict, Any, List
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_application_logger
from src.utils.config import get_test_settings
from src.main import RAGPipeline


class PipelineTester:
    """íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.logger = setup_application_logger("pipeline_test")
        self.project_root = Path(__file__).parent.parent
        self.test_results = {}
        self.temp_dir = None

    def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        try:
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            self.temp_dir = tempfile.mkdtemp(prefix="pipeline_test_")
            self.logger.info(f"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬: {self.temp_dir}")

            # í…ŒìŠ¤íŠ¸ ì„¤ì •
            settings = self._prepare_test_settings()

            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            tests = [
                ("í™˜ê²½ ì„¤ì •", self.test_environment),
                ("íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”", lambda: self.test_pipeline_initialization(settings)),
                ("ë¬¸ì„œ ì²˜ë¦¬", self.test_document_processing),
                ("ë²¡í„° ì €ì¥", self.test_vector_storage),
                ("ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰", self.test_context_retrieval),
                ("ë¬¸ì œ ìƒì„±", self.test_question_generation),
                ("ë°°ì¹˜ ì²˜ë¦¬", self.test_batch_processing),
                ("ì„±ëŠ¥ ì¸¡ì •", self.test_performance),
                ("ì—ëŸ¬ ì²˜ë¦¬", self.test_error_handling)
            ]

            for test_name, test_func in tests:
                self.logger.info(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘: {test_name}")
                try:
                    start_time = time.time()
                    result = test_func()
                    end_time = time.time()

                    self.test_results[test_name] = {
                        'status': 'success',
                        'result': result,
                        'duration': end_time - start_time
                    }
                    self.logger.info(f"âœ… {test_name} ì„±ê³µ ({end_time - start_time:.2f}ì´ˆ)")

                except Exception as e:
                    self.test_results[test_name] = {
                        'status': 'failed',
                        'error': str(e),
                        'duration': 0
                    }
                    self.logger.error(f"âŒ {test_name} ì‹¤íŒ¨: {str(e)}")

            # ê²°ê³¼ ìš”ì•½
            self._print_test_summary()

            return self.test_results

        finally:
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if self.temp_dir and Path(self.temp_dir).exists():
                import shutil
                shutil.rmtree(self.temp_dir, ignore_errors=True)

    def _prepare_test_settings(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì • ì¤€ë¹„"""
        settings = get_test_settings()
        settings.chroma_db_path = str(Path(self.temp_dir) / "vector_db")
        settings.cache_dir = str(Path(self.temp_dir) / "cache")

        # OpenAI API í‚¤ í™•ì¸
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key or api_key == 'your_openai_api_key_here':
            self.logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Mock ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            settings.openai_api_key = "sk-test-key-for-testing"
        else:
            settings.openai_api_key = api_key

        return settings

    def test_environment(self) -> Dict[str, Any]:
        """í™˜ê²½ í…ŒìŠ¤íŠ¸"""
        result = {}

        # Python ë²„ì „ í™•ì¸
        result['python_version'] = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"

        # í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
        required_packages = ['openai', 'chromadb', 'click', 'pydantic', 'tiktoken']
        missing_packages = []

        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)

        result['missing_packages'] = missing_packages

        # ë””ë ‰í† ë¦¬ ì ‘ê·¼ ê¶Œí•œ í™•ì¸
        test_file = Path(self.temp_dir) / "access_test.txt"
        try:
            test_file.write_text("test")
            test_file.unlink()
            result['file_access'] = True
        except Exception as e:
            result['file_access'] = False
            result['file_access_error'] = str(e)

        if missing_packages:
            raise Exception(f"í•„ìˆ˜ íŒ¨í‚¤ì§€ ëˆ„ë½: {missing_packages}")

        return result

    def test_pipeline_initialization(self, settings) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        from unittest.mock import patch

        # OpenAI API í˜¸ì¶œì„ Mockìœ¼ë¡œ ëŒ€ì²´
        with patch('src.rag.embeddings.openai.OpenAI'), \
             patch('src.models.llm_client.openai.OpenAI'):

            pipeline = RAGPipeline(settings)

            result = {
                'components_initialized': True,
                'vector_store_ready': pipeline.vector_store is not None,
                'llm_client_ready': pipeline.llm_client is not None,
                'question_generator_ready': pipeline.question_generator is not None
            }

            # ì €ì¥ ë³€ìˆ˜
            self.pipeline = pipeline

            return result

    def test_document_processing(self) -> Dict[str, Any]:
        """ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
        test_content = """ì¼ì°¨í•¨ìˆ˜ì˜ ì •ì˜

ì¼ì°¨í•¨ìˆ˜ëŠ” y = ax + b (a â‰  0) í˜•íƒœë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
ì—¬ê¸°ì„œ aëŠ” ê¸°ìš¸ê¸°, bëŠ” yì ˆí¸ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ì¼ì°¨í•¨ìˆ˜ì˜ ê·¸ë˜í”„ëŠ” ì§ì„ ì…ë‹ˆë‹¤.
ê¸°ìš¸ê¸°ê°€ ì–‘ìˆ˜ì´ë©´ ìš°ìƒí–¥í•˜ê³ , ìŒìˆ˜ì´ë©´ ìš°í•˜í–¥í•©ë‹ˆë‹¤.

ì˜ˆì‹œ:
y = 2x + 3 (ê¸°ìš¸ê¸° 2, yì ˆí¸ 3)
y = -x + 5 (ê¸°ìš¸ê¸° -1, yì ˆí¸ 5)"""

        test_file = Path(self.temp_dir) / "test_textbook.txt"
        test_file.write_text(test_content, encoding='utf-8')

        # ë¬¸ì„œ ì²˜ë¦¬
        processor = self.pipeline.document_processor
        documents = processor.load_textbook(str(test_file), "ìˆ˜í•™", "ì¼ì°¨í•¨ìˆ˜")

        result = {
            'total_documents': len(documents),
            'has_metadata': all('subject' in doc.metadata for doc in documents),
            'content_preserved': any('ì¼ì°¨í•¨ìˆ˜' in doc.content for doc in documents),
            'chunking_worked': len(documents) > 0
        }

        # ì €ì¥
        self.test_documents = documents

        return result

    def test_vector_storage(self) -> Dict[str, Any]:
        """ë²¡í„° ì €ì¥ í…ŒìŠ¤íŠ¸"""
        from unittest.mock import patch, MagicMock

        # ì„ë² ë”© ìƒì„± Mock
        with patch.object(self.pipeline.embeddings_manager, 'generate_embeddings') as mock_embed:
            mock_embed.return_value = [[0.1] * 1536 for _ in self.test_documents]

            # ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
            embeddings = mock_embed.return_value
            success = self.pipeline.vector_store.add_documents(self.test_documents, embeddings)

            # ì €ì¥ í™•ì¸
            info = self.pipeline.vector_store.get_collection_info()

            result = {
                'storage_success': success,
                'stored_documents': info['total_documents'],
                'subjects_stored': len(info['subjects']),
                'units_stored': len(info['units'])
            }

            return result

    def test_context_retrieval(self) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        from unittest.mock import patch

        # ì„ë² ë”© ìƒì„± Mock
        with patch.object(self.pipeline.embeddings_manager, 'generate_single_embedding') as mock_embed:
            mock_embed.return_value = [0.1] * 1536

            # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            contexts = self.pipeline.retriever.retrieve_context(
                query="ì¼ì°¨í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°",
                subject="ìˆ˜í•™",
                unit="ì¼ì°¨í•¨ìˆ˜",
                k=2
            )

            result = {
                'contexts_found': len(contexts),
                'relevant_content': any('ì¼ì°¨í•¨ìˆ˜' in context for context in contexts),
                'search_worked': len(contexts) > 0
            }

            return result

    def test_question_generation(self) -> Dict[str, Any]:
        """ë¬¸ì œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        from unittest.mock import patch, MagicMock

        # LLM ì‘ë‹µ Mock
        mock_response = {
            "question": "ì¼ì°¨í•¨ìˆ˜ y = 2x + 3ì—ì„œ ê¸°ìš¸ê¸°ëŠ” ë¬´ì—‡ì¸ê°€?",
            "options": ["1", "2", "3", "4", "5"],
            "correct_answer": 2,
            "explanation": "ì¼ì°¨í•¨ìˆ˜ y = ax + bì—ì„œ aê°€ ê¸°ìš¸ê¸°ì´ë¯€ë¡œ ë‹µì€ 2ì…ë‹ˆë‹¤.",
            "difficulty": "medium",
            "subject": "ìˆ˜í•™",
            "unit": "ì¼ì°¨í•¨ìˆ˜"
        }

        with patch.object(self.pipeline.llm_client, 'generate_structured_response') as mock_llm, \
             patch.object(self.pipeline.embeddings_manager, 'generate_single_embedding') as mock_embed:

            mock_llm.return_value = mock_response
            mock_embed.return_value = [0.1] * 1536

            # ë¬¸ì œ ìƒì„±
            question = self.pipeline.question_generator.generate_question(
                subject="ìˆ˜í•™",
                unit="ì¼ì°¨í•¨ìˆ˜",
                difficulty="medium"
            )

            # ê²€ì¦
            is_valid = self.pipeline.question_generator.validate_question(question)

            result = {
                'question_generated': question is not None,
                'has_correct_format': is_valid,
                'has_5_options': len(question.get('options', [])) == 5,
                'has_explanation': bool(question.get('explanation', '')),
                'correct_answer_valid': 1 <= question.get('correct_answer', 0) <= 5
            }

            # ì €ì¥
            self.test_question = question

            return result

    def test_batch_processing(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        from unittest.mock import patch

        mock_response = {
            "question": "í…ŒìŠ¤íŠ¸ ë¬¸ì œ",
            "options": ["1", "2", "3", "4", "5"],
            "correct_answer": 1,
            "explanation": "í…ŒìŠ¤íŠ¸ í•´ì„¤",
            "difficulty": "medium",
            "subject": "ìˆ˜í•™",
            "unit": "ì¼ì°¨í•¨ìˆ˜"
        }

        with patch.object(self.pipeline.llm_client, 'generate_structured_response') as mock_llm, \
             patch.object(self.pipeline.embeddings_manager, 'generate_single_embedding') as mock_embed:

            mock_llm.return_value = mock_response
            mock_embed.return_value = [0.1] * 1536

            # ë°°ì¹˜ ë¬¸ì œ ìƒì„±
            questions = self.pipeline.question_generator.generate_batch_questions(
                subject="ìˆ˜í•™",
                unit="ì¼ì°¨í•¨ìˆ˜",
                count=3,
                difficulty="medium"
            )

            result = {
                'batch_count': len(questions),
                'all_valid': all(self.pipeline.question_generator.validate_question(q) for q in questions),
                'unique_questions': len(set(q['question'] for q in questions)) == len(questions)
            }

            return result

    def test_performance(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        from unittest.mock import patch

        # ë§ì€ ë¬¸ì„œë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        large_documents = []
        for i in range(20):
            from src.rag.document_processor import Document
            large_documents.append(Document(
                content=f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. " * 10,
                metadata={"subject": "ìˆ˜í•™", "unit": f"ë‹¨ì›{i}", "index": i}
            ))

        # ì €ì¥ ì„±ëŠ¥ ì¸¡ì •
        with patch.object(self.pipeline.embeddings_manager, 'generate_embeddings') as mock_embed:
            mock_embed.return_value = [[0.1 + i * 0.01] * 1536 for i in range(len(large_documents))]

            start_time = time.time()
            embeddings = mock_embed.return_value
            success = self.pipeline.vector_store.add_documents(large_documents, embeddings)
            storage_time = time.time() - start_time

        # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
        with patch.object(self.pipeline.embeddings_manager, 'generate_single_embedding') as mock_embed:
            mock_embed.return_value = [0.1] * 1536

            start_time = time.time()
            contexts = self.pipeline.retriever.retrieve_context("í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬", k=5)
            search_time = time.time() - start_time

        result = {
            'storage_time': storage_time,
            'search_time': search_time,
            'documents_stored': len(large_documents),
            'storage_success': success,
            'search_results': len(contexts),
            'performance_acceptable': storage_time < 5.0 and search_time < 1.0
        }

        return result

    def test_error_handling(self) -> Dict[str, Any]:
        """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        from unittest.mock import patch

        result = {}

        # 1. ì˜ëª»ëœ íŒŒì¼ ê²½ë¡œ
        try:
            self.pipeline.document_processor.load_textbook("nonexistent.txt", "ìˆ˜í•™", "ì¼ì°¨í•¨ìˆ˜")
            result['file_error_handling'] = False
        except Exception:
            result['file_error_handling'] = True

        # 2. ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
        with patch.object(self.pipeline.retriever, 'retrieve_context', return_value=[]):
            try:
                self.pipeline.question_generator.generate_question("ìˆ˜í•™", "ì¼ì°¨í•¨ìˆ˜", "medium")
                result['empty_context_handling'] = False
            except Exception:
                result['empty_context_handling'] = True

        # 3. ì˜ëª»ëœ ë¬¸ì œ í˜•ì‹ ê²€ì¦
        invalid_question = {
            "question": "",  # ë¹ˆ ë¬¸ì œ
            "options": ["1", "2"],  # ë¶€ì¡±í•œ ì„ íƒì§€
            "correct_answer": 6,  # ì˜ëª»ëœ ì •ë‹µ
        }
        result['question_validation'] = not self.pipeline.question_generator.validate_question(invalid_question)

        return result

    def _print_test_summary(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ§ª íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("="*60)

        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result['status'] == 'success')
        failed_tests = total_tests - passed_tests
        total_time = sum(result['duration'] for result in self.test_results.values())

        print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼:")
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
        print(f"   ì„±ê³µ: {passed_tests}")
        print(f"   ì‹¤íŒ¨: {failed_tests}")
        print(f"   ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")

        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ìƒì„¸:")
        for test_name, result in self.test_results.items():
            status_icon = "âœ…" if result['status'] == 'success' else "âŒ"
            print(f"   {status_icon} {test_name} ({result['duration']:.2f}ì´ˆ)")

            if result['status'] == 'failed':
                print(f"      ì˜¤ë¥˜: {result['error']}")

        if failed_tests == 0:
            print(f"\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        else:
            print(f"\nâš ï¸  {failed_tests}ê°œì˜ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        print("\nğŸ’¡ ì¶”ì²œ ì‚¬í•­:")
        if failed_tests > 0:
            print("   - ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”")
            print("   - OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            print("   - ì˜ì¡´ì„± íŒ¨í‚¤ì§€ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        else:
            print("   - ì‹¤ì œ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”")
            print("   - í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”")

        print("\n" + "="*60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    tester = PipelineTester()

    try:
        results = tester.run_all_tests()

        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        results_file = Path(tester.project_root) / "test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ ë°˜í™˜
        failed_count = sum(1 for result in results.values() if result['status'] == 'failed')
        return 0 if failed_count == 0 else 1

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return 1


if __name__ == "__main__":
    sys.exit(main())